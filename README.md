# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model. I used HyperDrive to optimise the hyperparameter of the model. Additionally used Azure AutoML to find the optimal model using same dataset and compare the resultes of the both the methods.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

The data used here is of direct marketing campaigns through phone calls for banking institution. The classification goal is to predict if customer will subscribe to term deposit (yes/no). Dataset consits of 32950 records with 20 independent variables out of which 10 are numeric features and 10 are categorical features. Additionally targetvariable is "y".

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best permforming model was the AutoML model with ID - AutoML_f6431fa8-cd51-40d7-817e-97ca693a2e1d_4, the accuracy was 0.91558 and algorithm used was VotingEnsemble.
In Compared to the Scikit-learn HyperDriver model with ID - HD_6606dab1-d34f-429d-8535-6b44ff6a8ab9_29 and accuracy was 0.90789


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

![Pipeline diagram](/Images/Pipeline.jfif)

Pipeline architecture:<br/>
		Dataset - Created the dataset using TabularDatasetFactory. Split data into train and test (0.2) sets.<br/>
		Train data - Created the training script to train the marketing data dataset using Scikit-learn Logistic regression algorithm.<br/>
		HyperDrive - Used HyperDrive with specified parameter sampler and policy for early stopping to find the optimal hyperparameter for logistic regression model. This will give us trained model with optimize hyperparameter.<br/>
	

**What are the benefits of the parameter sampler you chose?**

I chose Random Paramer Sampler:</br>
	Random sampling supports discrete and continuous hyperparameters. It supports early termination of low-performance runs. It is computationally less expensive as it takes subset of combinations and it's faster unlike GridParameterSampling. Some users do an initial search with random sampling and then refine the search space to improve results. In random sampling, hyperparameter values are randomly selected from the defined search space.
	You can also specify the maximum number of runs that you want the module to execute. This option is useful when you want to increase model performance by using the metrics of your choice but still conserve computing resources. GridParameterSampling utilize more resources compare to RandomParameterSampling. 

ps = RandomParameterSampling( 
    {
        "--max_iter": choice(10,50,100,150,200)
        ,"--C": choice(0.5,0.8,0.9,1,1.25,1.5)
    }
)

Here i chose discrete values with _choice_ for both parameters, _C_ and _max_iter_. _C_ is the Regularization and _max_iter_ is the maximum number of iterations. 
This option trains a model by using a set number of iterations. You specify a range of values to iterate over, and the module uses a randomly chosen subset of those values. Values are chosen with replacement, meaning that numbers previously chosen at random are not removed from the pool of available numbers. So the chance of any value being selected stays the same across all passes.


**What are the benefits of the early stopping policy you chose?**

Early stopping policy automatically terminates poorly performing runs. For more aggressive savings, used Bandit Policy with a smaller allowable slack or Truncation Selection Policy with a larger truncation percentage. Any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated. This means that with this policy, the best performing runs will execute until they finish.

policy = BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=5)

_evaluation_interval_: (optional) the frequency for applying the policy. Each time the training script logs the primary metric counts as one interval.

_slack_factor_: The amount of slack allowed with respect to the best performing training run. This factor specifies the slack as a ratio.

_delay_evaluation: (optional) delays the first policy evaluation for a specified number of intervals.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

<b>Auto ML:</b></br>
	Dataset - Created Tabular dataset.</br>
	Auto ML - Used Auto ML to find best training model.</br>
	Description: Best Model generated was of VotingEnsemble algorithm. It is an ensemble created from previous AutoML iterations that implements soft voting. AutoML uses predicts based on the weighted average of predicted class probabilities. As our data is imbalan, Voting ensemble to avoid bias towards particular class.</br>

Below is the AutoML configuration i set for this project.

automl_config = AutoMLConfig(
    compute_target=compute_target,
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric="accuracy",
    training_data=train_data,
    label_column_name="y",
    n_cross_validations=5
)

_experiment_timeout_minutes=30_

This is an exit criterion and is used to define how long (in minutes), the experiment should continue to run. To help avoid experiment time out failures, I used the minimum of 20 minutes.

_task='classification'_

This defines the experiment type which in this case is classification.

_primary_metric='accuracy'_

I chose accuracy as the primary metric for this classification model.

_n_cross_validations=5_

This parameter sets how many cross validations to perform, based on the same number of folds (subsets). Five folds for cross-validation are defined. So, five different trainings, each training using 4/5 of the data, and each validation using 1/5 of the data with a different holdout fold each time.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

<b>HyperDrive Model:</b></br>
run_id - HD_6606dab1-d34f-429d-8535-6b44ff6a8ab9_29</br>
Accuracy - 0.9078907435508345</br>
Parameter sampling - Random</br>
Termination Policy - BANDIT</br>

<b>AutoML Model:</b></br>
run_id = AutoML_f6431fa8-cd51-40d7-817e-97ca693a2e1d_4</br>
Accuracy - 0.915585873177552</br>
Algorithm - VotingEnsemble</br>

The difference is the accuracy as above between two models. AutoML runs data agains multiple alogorithm so we get best model with higher accuracy compare to Hyperdrive model where we are working with one model. In every AutoML experiment, automatic scaling and normalization techniques are applied to your data by default. These techniques are types of featurization that help certain algorithms that are sensitive to features on different scales. You can enable more featurization, such as missing-values imputation, encoding, and transforms, in case of HyperDrive model we have to tune model with multiple runs.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

- For HyperDrive model, a range of different optimization algorithms can be used, like grid search as we did random search. Define a new model and set the hyperparameter values of the model to the values found by different search. Then fit the model on all available data and use the model to start making predictions on new data.
- Accuracy metrics is not suitable metric for Imbalanced data, rather AUC_weighted metric is best for imbalance data.
- We can explore more on using customize featurization settings to ensure that the data and features that are used to train your ML model result in relevant predictions.


## Proof of cluster clean up
**Image of cluster marked for deletion**

![Pipeline diagram](/Images/DeleteCompute.png)
![Pipeline diagram](/Images/DeleteCompute1.png)

