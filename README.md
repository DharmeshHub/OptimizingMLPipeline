# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model. I used HyperDrive to optimise the hyperparameter of the model. Additionally used Azure AutoML to find the optimal model using same dataset and compare the resultes of the both the methods.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

The data used here is of direct marketing campaigns through phone calls for banking institution. The classification goal is to predict if customer will subscribe to term deposit (yes/no). Dataset consits of 32950 records with 20 independent variables out of which 10 are numeric features and 10 are categorical features. Additionally targetvariable is "y".

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best permforming model was the AutoML model with ID - AutoML_f6431fa8-cd51-40d7-817e-97ca693a2e1d_4, the accuracy was 0.91558 and algorithm used was VotingEnsemble.
In Compared to the Scikit-learn HyperDriver model with ID - HD_6606dab1-d34f-429d-8535-6b44ff6a8ab9_29 and accuracy was 0.90789


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

![Pipeline diagram](/Images/Pipeline.jfif)

Pipeline architecture:<br/>
		Dataset - Created the dataset using TabularDatasetFactory. Split data into train and test (0.2) sets.<br/>
		Train data - Created the training script to train the marketing data dataset using Scikit-learn Logistic regression algorithm.<br/>
		HyperDrive - Used HyperDrive with specified parameter sampler and policy for early stopping to find the optimal hyperparameter for logistic regression model. This will give us trained model with optimize hyperparameter.<br/>
	

**What are the benefits of the parameter sampler you chose?**

I chose Random Paramer Sampler:</br>
	Random sampling supports discrete and continuous hyperparameters. It supports early termination of low-performance runs. It is computationally less expensive as it takes subset of combinations and it's faster unlike GridParameterSampling. Some users do an initial search with random sampling and then refine the search space to improve results. In random sampling, hyperparameter values are randomly selected from the defined search space.
	You can also specify the maximum number of runs that you want the module to execute. This option is useful when you want to increase model performance by using the metrics of your choice but still conserve computing resources. GridParameterSampling utilize more resources compare to RandomParameterSampling. 

ps = RandomParameterSampling( 
    {
        "--max_iter": choice(10,50,100,150,200)
        ,"--C": choice(0.5,0.8,0.9,1,1.25,1.5)
    }
)

Here i chose discrete values with _choice_ for both parameters, _C_ and _max_iter_. _C_ is the Regularization and _max_iter_ is the maximum number of iterations. 
This option trains a model by using a set number of iterations. You specify a range of values to iterate over, and the module uses a randomly chosen subset of those values. Values are chosen with replacement, meaning that numbers previously chosen at random are not removed from the pool of available numbers. So the chance of any value being selected stays the same across all passes.


**What are the benefits of the early stopping policy you chose?**

Early stopping policy automatically terminates poorly performing runs. For more aggressive savings, used Bandit Policy with a smaller allowable slack or Truncation Selection Policy with a larger truncation percentage. Any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated. This means that with this policy, the best performing runs will execute until they finish.

policy = BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=5)

_evaluation_interval_: (optional) the frequency for applying the policy. Each time the training script logs the primary metric counts as one interval.

_slack_factor_: The amount of slack allowed with respect to the best performing training run. This factor specifies the slack as a ratio.

_delay_evaluation: (optional) delays the first policy evaluation for a specified number of intervals.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

<b>Auto ML:</b></br>
	Dataset - Created Tabular dataset.</br>
	Auto ML - Used Auto ML to find best training model.</br>
	Description: Best Model generated was of VotingEnsemble algorithm. It is an ensemble created from previous AutoML iterations that implements soft voting. AutoML uses predicts based on the weighted average of predicted class probabilities. As our data is imbalan, Voting ensemble to avoid bias towards particular class.</br>

Below is the AutoML configuration i set for this project.

automl_config = AutoMLConfig(
    compute_target=compute_target,
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric="accuracy",
    training_data=train_data,
    label_column_name="y",
    n_cross_validations=5
)

_experiment_timeout_minutes=30_

This is an exit criterion and is used to define how long (in minutes), the experiment should continue to run. To help avoid experiment time out failures, I used the minimum of 20 minutes.

_task='classification'_

This defines the experiment type which in this case is classification.

_primary_metric='accuracy'_

I chose accuracy as the primary metric for this classification model.

_n_cross_validations=5_

This parameter sets how many cross validations to perform, based on the same number of folds (subsets). Five folds for cross-validation are defined. So, five different trainings, each training using 4/5 of the data, and each validation using 1/5 of the data with a different holdout fold each time.


<B>Parameters generated by AutoML</B>

AutoML used Voting Classifier Ensemble methods, there are two types of voting method a) Soft and b) Hard voting. Hard voting is basically majority carries the vote. Here in our AutoML uses Soft voting, it predicts the class label based on argmax of the sums of the predicted probabilities of individual models that make up the ensemble. i.e weighted average the probabilities.
AutoML set below parameters to avoid over-fitting.

('prefittedsoftvotingclassifier',...\n",
            "                                                                                                    min_samples_leaf=0.01,\n",
            "                                                                                                    min_samples_split=0.2442105263157895,\n",
            "                                                                                                    min_weight_fraction_leaf=0.0,\n",
            "                                                                                                    n_estimators=10,\n",
            "                                                                                                    n_jobs=1,\n",
            "                                                                                                    oob_score=False,\n",
            "                                                                                                    random_state=None,\n",
            "                                                                                                    verbose=0,\n",
            "                                                                                                    warm_start=False))],\n",
            "                                                                     verbose=False))],\n",
			


min_samples_split - An internal node will have further splits, this specifies the minimum number of sample required to split an internal node. We can specifiy a number to denote the minimum number or a fraction to denote the percentage of samples in an internal node.

min_samples_leaf - A leaf node is a node without any further splits. This specifies the minimum number of samples required to be a leaf nodes.


min_weight_fraction_leaf - Default=0.0
This is quite similar to min_samples_leaf, but it uses a fraction of the sum total number of observations instead.

n_estimators - Default=10
The number of trees your want to build within a Random Forest before aggregating the predictions. The higher the number the better, but it is important to know this is more computationally expensive and will take longer for your code to run.
 
n_jobs-(integer)-Default=1
This lets the computer know how many processors it is allowed to use. The default value of 1 means it can only use one processor. If you use -1 it means that there is no restriction of how much processing power the code can use. Setting your n_jobs to -1 will often lead to faster processing.

oob_score-(boolean)-Default=False
This is a cross-validation method that is very similar to a leave-one out validation technique where the generalized estimated performance of a model is trained on n-1 samples of the data. However, oob_score is much faster because it grabs all observations used in the trees and finds out the maximum score for each samples base on the trees which did not use that samples to train.

random_state-(integer, RandomState instance, None)-Default=None
Since the bootstrapping generates random samples it is often hard to exactly duplicate results. This parameter makes it easy for others to replicate your results if given the same training data and parameters.

verbose-(integer)-Default=0
Verbose means that you are setting the logging output which gives you constant updates about what the model is doing as it processed. This parameter sets the verbosity of the tree’s building process. It is not always useful and may take up an unnecessary space in your notebook.

warm_start-(boolean)-Default=False
At False it fits a new forest each time as opposed to when it is True it adds estimators and reuses the solution of the previous fit. It is mostly used when you are using recursive feature selection. This means that when you drop some features other features will gain in importance and to “appreciate” the trees they must be reused. It is often used with backward elimination in regression models and not often used in classification models



## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

<b>HyperDrive Model:</b></br>
run_id - HD_6606dab1-d34f-429d-8535-6b44ff6a8ab9_29</br>
Accuracy - 0.9078907435508345</br>
Parameter sampling - Random</br>
Termination Policy - BANDIT</br>

<b>AutoML Model:</b></br>
run_id = AutoML_f6431fa8-cd51-40d7-817e-97ca693a2e1d_4</br>
Accuracy - 0.915585873177552</br>
Algorithm - VotingEnsemble</br>

The difference is the accuracy as above between two models. AutoML runs data agains multiple alogorithm so we get best model with higher accuracy compare to Hyperdrive model where we are working with one model. In every AutoML experiment, automatic scaling and normalization techniques are applied to your data by default. These techniques are types of featurization that help certain algorithms that are sensitive to features on different scales. You can enable more featurization, such as missing-values imputation, encoding, and transforms, in case of HyperDrive model we have to tune model with multiple runs.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

- For HyperDrive model, a range of different optimization algorithms can be used, like grid search as we did random search. Define a new model and set the hyperparameter values of the model to the values found by different search. Then fit the model on all available data and use the model to start making predictions on new data.
- Accuracy metrics is not suitable metric for Imbalanced data, rather AUC_weighted metric is best for imbalance data.
- We can explore more on using customize featurization settings to ensure that the data and features that are used to train your ML model result in relevant predictions.


## Proof of cluster clean up
**Image of cluster marked for deletion**

![Pipeline diagram](/Images/DeleteCompute.png)
![Pipeline diagram](/Images/DeleteCompute1.png)

